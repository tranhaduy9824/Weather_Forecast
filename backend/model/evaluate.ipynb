{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model epoch 4 no embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "30250/30250 [==============================] - 901s 29ms/step - loss: 0.0054 - mae: 0.0264\n",
      "‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n d·ªØ li·ªáu test: Loss=0.0054, MAE=0.0264\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "\n",
    "checkpoint_path = \"../model/best_model2.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "loss, mae = model.evaluate(test_dataset)\n",
    "print(f\"‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n d·ªØ li·ªáu test: Loss={loss:.4f}, MAE={mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune & CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...\n",
      "30250/30250 [==============================] - 689s 23ms/step - loss: 0.0049 - mae: 0.0234\n",
      "‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n d·ªØ li·ªáu test: Loss=0.0049, MAE=0.0234\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "checkpoint_path = \"best_model_spatial.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\", \"Latitude\", \"Longitude\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\nƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...\")\n",
    "loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n d·ªØ li·ªáu test: Loss={loss:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...\n",
      "30250/30250 [==============================] - 739s 24ms/step - loss: 0.0046 - mae: 0.0223\n",
      "‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n d·ªØ li·ªáu test: Loss=0.0046, MAE=0.0223\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "checkpoint_path = \"best_model_spatial.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\", \"Latitude\", \"Longitude\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\nƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...\")\n",
    "loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n d·ªØ li·ªáu test: Loss={loss:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reorder_and_clean_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 140\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../dataset/scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    138\u001b[0m     scaler_X, scaler_y \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m--> 140\u001b[0m forecast_results \u001b[38;5;241m=\u001b[39m \u001b[43mforecast_for_all_lat_lon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_day_20_unscaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(forecast_results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    143\u001b[0m     forecast_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m forecast_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 87\u001b[0m, in \u001b[0;36mforecast_for_all_lat_lon\u001b[1;34m(df_day_20, scaler_X, scaler_y)\u001b[0m\n\u001b[0;32m     84\u001b[0m forecast_results_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lat, lon \u001b[38;5;129;01min\u001b[39;00m available_coords:\n\u001b[1;32m---> 87\u001b[0m     forecast_df, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_model_for_lat_lon\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_day_20\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m forecast_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m         forecast_results_list\u001b[38;5;241m.\u001b[39mappend(forecast_df)\n",
      "Cell \u001b[1;32mIn[10], line 51\u001b[0m, in \u001b[0;36mpredict_with_model_for_lat_lon\u001b[1;34m(df_day_20, lat, lon, scaler_X, scaler_y, model)\u001b[0m\n\u001b[0;32m     48\u001b[0m df_lat_lon \u001b[38;5;241m=\u001b[39m add_time_features(df_lat_lon)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# ƒê·∫£m b·∫£o c·ªôt PRECTOTCORR c√≥ m·∫∑t\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m df_lat_lon \u001b[38;5;241m=\u001b[39m \u001b[43mreorder_and_clean_columns\u001b[49m(df_lat_lon)\n\u001b[0;32m     53\u001b[0m X_input, _ \u001b[38;5;241m=\u001b[39m create_lstm_input(df_lat_lon, time_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reorder_and_clean_columns' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========================== LOAD & CLEAN DATA ==========================\n",
    "\n",
    "def load_processed_data():\n",
    "    df_day_20 = pd.read_csv('../dataset/real_weather_data.csv')\n",
    "    df_day_21 = pd.read_csv('../dataset/real_weather_data.csv')\n",
    "    return df_day_20, df_day_21\n",
    "\n",
    "def add_time_features(df):\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')\n",
    "    df['hour'] = df['Datetime'].dt.hour\n",
    "    df['day'] = df['Datetime'].dt.day\n",
    "    df['month'] = df['Datetime'].dt.month\n",
    "    df['season'] = (df['month'] % 12 + 3) // 3\n",
    "    return df\n",
    "\n",
    "# ========================== LSTM INPUT ==========================\n",
    "\n",
    "def create_lstm_input(df, time_steps=24):\n",
    "    X, y = [], []\n",
    "    features = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "\n",
    "    for i in range(len(df) - time_steps - 24 + 1):\n",
    "        X_window = df.iloc[i:i+time_steps][features].values\n",
    "        y_window = df.iloc[i+time_steps:i+time_steps+24][features[:6]].values\n",
    "        if y_window.shape[0] == 24:\n",
    "            X.append(X_window)\n",
    "            y.append(y_window)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ========================== FORECAST ==========================\n",
    "\n",
    "def predict_with_model_for_lat_lon(df_day_20, lat, lon, scaler_X, scaler_y, model):\n",
    "    df_lat_lon = df_day_20[(df_day_20['Latitude'] == lat) & (df_day_20['Longitude'] == lon)].copy()\n",
    "    if len(df_lat_lon) == 0:\n",
    "        print(f\"No data for lat {lat}, lon {lon}\")\n",
    "        return None, None\n",
    "\n",
    "    # Th√™m c·ªôt th·ªùi gian\n",
    "    df_lat_lon = add_time_features(df_lat_lon)\n",
    "    \n",
    "    # ƒê·∫£m b·∫£o c·ªôt PRECTOTCORR c√≥ m·∫∑t\n",
    "    df_lat_lon = reorder_and_clean_columns(df_lat_lon)\n",
    "    \n",
    "    X_input, _ = create_lstm_input(df_lat_lon, time_steps=24)\n",
    "    if X_input.shape[0] == 0:\n",
    "        print(f\"Not enough data for prediction at lat {lat}, lon {lon}\")\n",
    "        return None, None\n",
    "\n",
    "    features = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "\n",
    "    X_input_df = pd.DataFrame(X_input.reshape(-1, len(features)), columns=features)\n",
    "    X_input_df = X_input_df[scaler_X.feature_names_in_]\n",
    "    X_input_scaled = scaler_X.transform(X_input_df).reshape(X_input.shape)\n",
    "\n",
    "    y_pred_scaled = model.predict(X_input_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled[-1])  # 24 d√≤ng cu·ªëi\n",
    "\n",
    "    forecast_start = df_lat_lon['Datetime'].iloc[-1] + pd.Timedelta(hours=1)\n",
    "    forecast_times = pd.date_range(start=forecast_start, periods=24, freq='h')\n",
    "\n",
    "    forecast_df = pd.DataFrame(y_pred, columns=[\n",
    "        'Predicted_T2M', 'Predicted_QV2M', 'Predicted_PS', 'Predicted_WS10M',\n",
    "        'Predicted_PRECTOTCORR', 'Predicted_CLRSKY_SFC_SW_DWN'\n",
    "    ])\n",
    "    forecast_df['Datetime'] = forecast_times\n",
    "    forecast_df['Latitude'] = lat\n",
    "    forecast_df['Longitude'] = lon\n",
    "\n",
    "    return forecast_df, y_pred\n",
    "\n",
    "def forecast_for_all_lat_lon(df_day_20, scaler_X, scaler_y):\n",
    "    model = load_model('best_model_fine_tune.h5')\n",
    "    available_coords = df_day_20[['Latitude', 'Longitude']].drop_duplicates().values\n",
    "    forecast_results_list = []\n",
    "\n",
    "    for lat, lon in available_coords:\n",
    "        forecast_df, _ = predict_with_model_for_lat_lon(df_day_20, lat, lon, scaler_X, scaler_y, model)\n",
    "        if forecast_df is not None:\n",
    "            forecast_results_list.append(forecast_df)\n",
    "\n",
    "    forecast_results = pd.concat(forecast_results_list, ignore_index=True)\n",
    "    return forecast_results\n",
    "\n",
    "# ========================== EVALUATE ==========================\n",
    "\n",
    "def evaluate_model(df_day_21, forecast_results):\n",
    "    df_day_21['Datetime'] = pd.to_datetime(df_day_21['Datetime'], errors='coerce')\n",
    "    forecast_results['Datetime'] = pd.to_datetime(forecast_results['Datetime'], errors='coerce')\n",
    "\n",
    "    df_day_21['Latitude'] = df_day_21['Latitude'].astype(np.float32)\n",
    "    df_day_21['Longitude'] = df_day_21['Longitude'].astype(np.float32)\n",
    "    forecast_results['Latitude'] = forecast_results['Latitude'].astype(np.float32)\n",
    "    forecast_results['Longitude'] = forecast_results['Longitude'].astype(np.float32)\n",
    "\n",
    "    merged = pd.merge(df_day_21, forecast_results,\n",
    "                      on=['Datetime', 'Latitude', 'Longitude'],\n",
    "                      how='inner', suffixes=('', '_pred'))\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        print(\"No matching data for evaluation\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n‚úÖ S·ªë l∆∞·ª£ng d√≤ng ƒë∆∞·ª£c so s√°nh: {len(merged)}\")\n",
    "    print(merged[['Datetime', 'Latitude', 'Longitude'] + [f'Predicted_{c}' for c in ['T2M', 'QV2M']]].head())\n",
    "\n",
    "    metrics = {}\n",
    "    for col in [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"]:  # S·ª≠a t√™n c·ªôt ·ªü ƒë√¢y\n",
    "        actual = merged[col]\n",
    "        pred = merged[f'Predicted_{col}']\n",
    "        metrics[col] = {\n",
    "            'MAE': mean_absolute_error(actual, pred),\n",
    "            'MSE': mean_squared_error(actual, pred),\n",
    "            'R2': r2_score(actual, pred)\n",
    "        }\n",
    "\n",
    "    print(\"\\nüîç Chi ti·∫øt c√°c ch·ªâ s·ªë ƒë√°nh gi√°:\\n\")\n",
    "    df_metrics = pd.DataFrame(metrics).T\n",
    "    df_metrics.columns = ['MAE', 'MSE', 'R2']\n",
    "    print(df_metrics.round(4))\n",
    "\n",
    "# ========================== MAIN ==========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_day_20, df_day_21 = load_processed_data()\n",
    "    df_day_20_unscaled = df_day_20.copy()\n",
    "\n",
    "    with open('../dataset/scaler.pkl', 'rb') as f:\n",
    "        scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "    forecast_results = forecast_for_all_lat_lon(df_day_20_unscaled, scaler_X, scaler_y)\n",
    "\n",
    "    if len(forecast_results) > 0:\n",
    "        forecast_results['Latitude'] = forecast_results['Latitude'].round(4)\n",
    "        forecast_results['Longitude'] = forecast_results['Longitude'].round(4)\n",
    "\n",
    "        evaluate_model(df_day_21, forecast_results)\n",
    "        \n",
    "        forecast_results.to_csv(\"forecast_day26.csv\", index=False)\n",
    "        print(\"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ d·ª± b√°o v√†o forecast_day26.csv\")\n",
    "\n",
    "        print(\"\\nüìä K·∫øt qu·∫£ d·ª± b√°o:\")\n",
    "        print(forecast_results.head())\n",
    "        print(\"üìà Kho·∫£ng th·ªùi gian d·ª± b√°o:\",\n",
    "              forecast_results['Datetime'].min(), \"‚Üí\", forecast_results['Datetime'].max())\n",
    "\n",
    "        print(\"\\nüìä D·ªØ li·ªáu th·ª±c t·∫ø:\")\n",
    "        print(df_day_21.head())\n",
    "        print(\"üìà Kho·∫£ng th·ªùi gian th·ª±c t·∫ø:\",\n",
    "              df_day_21['Datetime'].min(), \"‚Üí\", df_day_21['Datetime'].max())\n",
    "    else:\n",
    "        print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ d·ª± b√°o ƒë·ªÉ ƒë√°nh gi√°.\")\n",
    "        \n",
    "            # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± b√°o v√† th·ª±c t·∫ø chi ti·∫øt h∆°n\n",
    "        print(\"\\nüìã M·ªôt v√†i d√≤ng d·ªØ li·ªáu d·ª± b√°o:\")\n",
    "        print(forecast_results[['Datetime', 'Latitude', 'Longitude'] + [col for col in forecast_results.columns if col.startswith('Predicted_')]].head(10))\n",
    "\n",
    "        print(\"\\nüìã M·ªôt v√†i d√≤ng d·ªØ li·ªáu th·ª±c t·∫ø:\")\n",
    "        print(df_day_21[['Datetime', 'Latitude', 'Longitude', 'T2M', 'QV2M', 'PS', 'WS10M', 'PRECTOTCORR', 'CLRSKY_SFC_SW_DWN']].head(10))\n",
    "\n",
    "    # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh\n",
    "    def plot_results_for_location(df_actual, df_pred, lat, lon):\n",
    "        actual_data = df_actual[(df_actual['Latitude'] == lat) & (df_actual['Longitude'] == lon)]\n",
    "        pred_data = df_pred[(df_pred['Latitude'] == lat) & (df_pred['Longitude'] == lon)]\n",
    "\n",
    "        if actual_data.empty or pred_data.empty:\n",
    "            print(f\"Kh√¥ng c√≥ d·ªØ li·ªáu cho to·∫° ƒë·ªô ({lat}, {lon})\")\n",
    "            return\n",
    "\n",
    "        cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"]\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        for i, col in enumerate(cols):\n",
    "            plt.subplot(3, 2, i+1)\n",
    "            plt.plot(actual_data['Datetime'], actual_data[col], label='Th·ª±c t·∫ø', color='blue')\n",
    "            plt.plot(pred_data['Datetime'], pred_data[f'Predicted_{col}'], label='D·ª± b√°o', linestyle='--', color='red')\n",
    "            plt.title(f'{col} t·∫°i ({lat}, {lon})')\n",
    "            plt.xlabel('Th·ªùi gian')\n",
    "            plt.ylabel(col)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Ch·ªçn m·ªôt to·∫° ƒë·ªô ph·ªï bi·∫øn nh·∫•t ƒë·ªÉ v·∫Ω\n",
    "    most_common_coord = forecast_results.groupby(['Latitude', 'Longitude']).size().idxmax()\n",
    "    plot_results_for_location(df_day_21, forecast_results, *most_common_coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting weather forecasting pipeline...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "=== DEBUG SCALERS ===\n",
      "Scaler_X features: ['Latitude' 'Longitude' 'hour' 'day' 'month' 'season' 'WS10M' 'QV2M' 'PS'\n",
      " 'PRECTOTCORR' 'T2M' 'CLRSKY_SFC_SW_DWN']\n",
      "Scaler_X min: [   8.  102.    0.    1.    1.    1. -999. -999. -999.    0. -999. -999.]\n",
      "Scaler_X max: [  24.    118.     23.     31.     12.      4.   1072.65 1063.78 1031.07\n",
      " 2178.57 1087.62 1073.12]\n",
      "\n",
      "Scaler_y features: ['CLRSKY_SFC_SW_DWN' 'PS' 'T2M' 'QV2M' 'WS10M' 'PRECTOTCORR']\n",
      "Scaler_y min: [-999. -999. -999. -999. -999.    0.]\n",
      "Scaler_y max: [1073.12 1031.07 1087.62 1063.78 1072.65 2178.57]\n",
      "\n",
      "Loading training data...\n",
      "\n",
      "Loading test data...\n",
      "\n",
      "=== DEBUG TRAINING DATA SAMPLE ===\n",
      "Shape: (40, 13)\n",
      "             Datetime  Latitude  Longitude    T2M    PS  QV2M  WS10M  \\\n",
      "0 2025-04-01 06:00:00      21.0      105.8  21.02  1018    38   3.50   \n",
      "1 2025-04-01 09:00:00      21.0      105.8  22.03  1017    37   2.45   \n",
      "2 2025-04-01 12:00:00      21.0      105.8  20.79  1016    42   2.61   \n",
      "\n",
      "   PRECTOTCORR  CLRSKY_SFC_SW_DWN  hour  day  month  season  \n",
      "0          0.0                450     6    1      4       2  \n",
      "1          0.0                370     9    1      4       2  \n",
      "2          0.0                235    12    1      4       2  \n",
      "\n",
      "Data types:\n",
      "Datetime             datetime64[ns]\n",
      "Latitude                    float64\n",
      "Longitude                   float64\n",
      "T2M                         float64\n",
      "PS                            int64\n",
      "QV2M                          int64\n",
      "WS10M                       float64\n",
      "PRECTOTCORR                 float64\n",
      "CLRSKY_SFC_SW_DWN             int64\n",
      "hour                           int8\n",
      "day                            int8\n",
      "month                          int8\n",
      "season                         int8\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Datetime             0\n",
      "Latitude             0\n",
      "Longitude            0\n",
      "T2M                  0\n",
      "PS                   0\n",
      "QV2M                 0\n",
      "WS10M                0\n",
      "PRECTOTCORR          0\n",
      "CLRSKY_SFC_SW_DWN    0\n",
      "hour                 0\n",
      "day                  0\n",
      "month                0\n",
      "season               0\n",
      "dtype: int64\n",
      "\n",
      "=== DEBUG TEST DATA SAMPLE ===\n",
      "Shape: (40, 13)\n",
      "             Datetime  Latitude  Longitude    T2M    PS  QV2M  WS10M  \\\n",
      "0 2025-04-01 06:00:00      21.0      105.8  21.02  1018    38   3.50   \n",
      "1 2025-04-01 09:00:00      21.0      105.8  22.03  1017    37   2.45   \n",
      "2 2025-04-01 12:00:00      21.0      105.8  20.79  1016    42   2.61   \n",
      "\n",
      "   PRECTOTCORR  CLRSKY_SFC_SW_DWN  hour  day  month  season  \n",
      "0          0.0                450     6    1      4       2  \n",
      "1          0.0                370     9    1      4       2  \n",
      "2          0.0                235    12    1      4       2  \n",
      "\n",
      "Data types:\n",
      "Datetime             datetime64[ns]\n",
      "Latitude                    float64\n",
      "Longitude                   float64\n",
      "T2M                         float64\n",
      "PS                            int64\n",
      "QV2M                          int64\n",
      "WS10M                       float64\n",
      "PRECTOTCORR                 float64\n",
      "CLRSKY_SFC_SW_DWN             int64\n",
      "hour                           int8\n",
      "day                            int8\n",
      "month                          int8\n",
      "season                         int8\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "Datetime             0\n",
      "Latitude             0\n",
      "Longitude            0\n",
      "T2M                  0\n",
      "PS                   0\n",
      "QV2M                 0\n",
      "WS10M                0\n",
      "PRECTOTCORR          0\n",
      "CLRSKY_SFC_SW_DWN    0\n",
      "hour                 0\n",
      "day                  0\n",
      "month                0\n",
      "season               0\n",
      "dtype: int64\n",
      "\n",
      "=== MODEL EVALUATION ===\n",
      "\n",
      "Evaluating on test data...\n",
      "\n",
      "‚ùå Error: cannot convert float infinity to integer\n",
      "\n",
      "Pipeline completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\keras\\utils\\generic_utils.py:993: RuntimeWarning: divide by zero encountered in log10\n",
      "  numdigits = int(np.log10(self.target)) + 1\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_24376\\2277396659.py\", line 186, in main\n",
      "    evaluate_model()\n",
      "  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_24376\\2277396659.py\", line 151, in evaluate_model\n",
      "    evaluation_metrics = model.evaluate(test_dataset, verbose=1)\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 993, in update\n",
      "    numdigits = int(np.log10(self.target)) + 1\n",
      "OverflowError: cannot convert float infinity to integer\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "train_file = \"../dataset/real_weather_data.csv\"\n",
    "test_file = \"../dataset/real_weather_data.csv\"\n",
    "checkpoint_path = \"best_model_fine_tune.h5\"\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"]\n",
    "\n",
    "timesteps = 24  # Input sequence length\n",
    "forecast_steps = 24  # Must match model's output length\n",
    "batch_size = 256\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    try:\n",
    "        # Load scalers and model\n",
    "        try:\n",
    "            with open(scaler_file, 'rb') as f:\n",
    "                scaler_X, scaler_y = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Scaler file not found at {scaler_file}. Please ensure the file exists.\")\n",
    "\n",
    "        try:\n",
    "            model = load_model(checkpoint_path)\n",
    "            model.summary()  # Verify input/output shapes\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Model file not found at {checkpoint_path}. Please ensure the file exists.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading model: {str(e)}\")\n",
    "\n",
    "        def debug_scalers(scaler_X, scaler_y):\n",
    "            \"\"\"Print scaler information for debugging\"\"\"\n",
    "            print(\"\\n=== DEBUG SCALERS ===\")\n",
    "            print(\"Scaler_X features:\", scaler_X.feature_names_in_)\n",
    "            print(\"Scaler_X min:\", scaler_X.data_min_)\n",
    "            print(\"Scaler_X max:\", scaler_X.data_max_)\n",
    "            print(\"\\nScaler_y features:\", scaler_y.feature_names_in_)\n",
    "            print(\"Scaler_y min:\", scaler_y.data_min_)\n",
    "            print(\"Scaler_y max:\", scaler_y.data_max_)\n",
    "\n",
    "        def preprocess_data(df):\n",
    "            \"\"\"Clean and prepare raw data\"\"\"\n",
    "            if 'PRECTOT' in df.columns and 'PRECTOTCORR' not in df.columns:\n",
    "                df = df.rename(columns={'PRECTOT': 'PRECTOTCORR'})\n",
    "            \n",
    "            df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "            df['hour'] = df['Datetime'].dt.hour.astype(np.int8)\n",
    "            df['day'] = df['Datetime'].dt.day.astype(np.int8)\n",
    "            df['month'] = df['Datetime'].dt.month.astype(np.int8)\n",
    "            df['season'] = ((df['month'] % 12 + 3) // 3).astype(np.int8)\n",
    "            \n",
    "            # Handle missing values and invalid numbers\n",
    "            for col in feature_cols + target_cols:\n",
    "                if col in df.columns:\n",
    "                    if df[col].dtype == 'object':\n",
    "                        df[col] = pd.to_numeric(df[col].astype(str).str.extract('([-+]?\\d*\\.?\\d+)')[0], errors='coerce')\n",
    "                    df[col] = df[col].replace(-999, np.nan)\n",
    "                    df[col] = df[col].fillna(df[col].mean())\n",
    "            \n",
    "            return df\n",
    "\n",
    "        # Debugging information\n",
    "        debug_scalers(scaler_X, scaler_y)\n",
    "        \n",
    "        # Load and check data\n",
    "        print(\"\\nLoading training data...\")\n",
    "        try:\n",
    "            train_df = pd.read_csv(train_file)\n",
    "            train_df = preprocess_data(train_df)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Training file not found at {train_file}. Please ensure the file exists.\")\n",
    "        \n",
    "        print(\"\\nLoading test data...\")\n",
    "        try:\n",
    "            test_df = pd.read_csv(test_file)\n",
    "            test_df = preprocess_data(test_df)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Test file not found at {test_file}. Please ensure the file exists.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading test data: {str(e)}\")\n",
    "\n",
    "        def debug_data_sample(df, name=\"Data\"):\n",
    "            \"\"\"Print sample data for inspection\"\"\"\n",
    "            print(f\"\\n=== DEBUG {name.upper()} SAMPLE ===\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(df.head(3))\n",
    "            print(\"\\nData types:\")\n",
    "            print(df.dtypes)\n",
    "            print(\"\\nMissing values:\")\n",
    "            print(df.isna().sum())\n",
    "\n",
    "        debug_data_sample(train_df, \"Training Data\")\n",
    "        debug_data_sample(test_df, \"Test Data\")\n",
    "\n",
    "        def create_sequences(data, seq_length, target_length):\n",
    "            \"\"\"Create input-output sequences from time series data\"\"\"\n",
    "            X, y = [], []\n",
    "            for i in range(len(data) - seq_length - target_length + 1):\n",
    "                X.append(data[i:i+seq_length])\n",
    "                y.append(data[i+seq_length:i+seq_length+target_length])\n",
    "            return np.array(X), np.array(y)\n",
    "\n",
    "        def evaluation_data_generator(file_path):\n",
    "            \"\"\"Generate batches of evaluation data\"\"\"\n",
    "            dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "            dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "            \n",
    "            for chunk in pd.read_csv(file_path, chunksize=batch_size*5, dtype=dtype_dict, parse_dates=[\"Datetime\"]):\n",
    "                chunk = preprocess_data(chunk)\n",
    "                if len(chunk) < timesteps + forecast_steps:\n",
    "                    continue\n",
    "                \n",
    "                X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "                y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "                \n",
    "                # Create sequences with correct lengths\n",
    "                X_batch, y_batch = create_sequences(X_scaled, timesteps, forecast_steps)\n",
    "                y_target = create_sequences(y_scaled, timesteps, forecast_steps)[1]\n",
    "                \n",
    "                if len(X_batch) > 0:\n",
    "                    yield X_batch, y_target\n",
    "\n",
    "        def evaluate_model():\n",
    "            \"\"\"Evaluate model performance\"\"\"\n",
    "            print(\"\\n=== MODEL EVALUATION ===\")\n",
    "            \n",
    "            # Create dataset with proper shapes\n",
    "            test_dataset = tf.data.Dataset.from_generator(\n",
    "                lambda: evaluation_data_generator(test_file),\n",
    "                output_signature=(\n",
    "                    tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(None, forecast_steps, len(target_cols)), dtype=tf.float32)\n",
    "                )\n",
    "            ).prefetch(tf.data.AUTOTUNE)\n",
    "            \n",
    "            # Run evaluation\n",
    "            print(\"\\nEvaluating on test data...\")\n",
    "            evaluation_metrics = model.evaluate(test_dataset, verbose=1)\n",
    "            \n",
    "            if isinstance(evaluation_metrics, list):\n",
    "                print(\"\\nEvaluation metrics:\")\n",
    "                for metric_name, metric_value in zip(model.metrics_names, evaluation_metrics):\n",
    "                    print(f\"{metric_name}: {metric_value:.4f}\")\n",
    "            else:\n",
    "                print(f\"\\nEvaluation loss: {evaluation_metrics:.4f}\")\n",
    "            \n",
    "            # Sample predictions\n",
    "            print(\"\\nRunning sample predictions...\")\n",
    "            test_generator = evaluation_data_generator(test_file)\n",
    "            X_sample, y_sample_true = next(test_generator)\n",
    "            \n",
    "            if X_sample.size > 0:\n",
    "                y_sample_pred = model.predict(X_sample[:1], verbose=1)  # Predict first sample\n",
    "                \n",
    "                # Inverse transform scaling\n",
    "                y_true = scaler_y.inverse_transform(y_sample_true[0].reshape(-1, len(target_cols)))\n",
    "                y_pred = scaler_y.inverse_transform(y_sample_pred.reshape(-1, len(target_cols)))\n",
    "                \n",
    "                def debug_model_predictions(y_true, y_pred, target_names):\n",
    "                    \"\"\"Compare predictions with ground truth\"\"\"\n",
    "                    print(\"\\n=== DEBUG PREDICTIONS ===\")\n",
    "                    for i, col in enumerate(target_names):\n",
    "                        print(f\"\\n{col}:\")\n",
    "                        print(f\"- True values (sample): {y_true[:3,i]}\")\n",
    "                        print(f\"- Pred values (sample): {y_pred[:3,i]}\")\n",
    "                        print(f\"- MAE: {np.mean(np.abs(y_true[:,i] - y_pred[:,i])):.4f}\")\n",
    "                        print(f\"- RMSE: {np.sqrt(np.mean((y_true[:,i] - y_pred[:,i])**2)):.4f}\")\n",
    "                \n",
    "                debug_model_predictions(y_true, y_pred, target_cols)\n",
    "            else:\n",
    "                print(\"Warning: No valid samples found for prediction demonstration\")\n",
    "\n",
    "        evaluate_model()\n",
    "\n",
    "        def make_forecast(model, input_data):\n",
    "            \"\"\"Generate future predictions\"\"\"\n",
    "            print(\"\\n=== FORECASTING ===\")\n",
    "            input_data = preprocess_data(input_data)\n",
    "            \n",
    "            if len(input_data) < timesteps:\n",
    "                raise ValueError(f\"Need at least {timesteps} timesteps, got {len(input_data)}\")\n",
    "            \n",
    "            # Prepare input sequence\n",
    "            X_df = input_data[feature_cols].iloc[-timesteps:]\n",
    "            X_scaled = scaler_X.transform(X_df)\n",
    "            X_scaled = X_scaled.reshape(1, timesteps, len(feature_cols))\n",
    "            \n",
    "            # Make prediction\n",
    "            print(\"\\nMaking prediction...\")\n",
    "            y_pred_scaled = model.predict(X_scaled, verbose=1)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, len(target_cols)))\n",
    "            \n",
    "            # Generate forecast dates\n",
    "            last_date = input_data['Datetime'].iloc[-1]\n",
    "            forecast_dates = pd.date_range(\n",
    "                start=last_date + pd.Timedelta(hours=1),\n",
    "                periods=forecast_steps,\n",
    "                freq='h'\n",
    "            )\n",
    "            \n",
    "            # Create result DataFrame\n",
    "            result = pd.DataFrame({\n",
    "                'Datetime': forecast_dates,\n",
    "                **{f'Predicted_{col}': y_pred[:, i] for i, col in enumerate(target_cols)}\n",
    "            })\n",
    "            \n",
    "            print(\"\\nForecast summary:\")\n",
    "            print(result.describe())\n",
    "            \n",
    "            return result\n",
    "\n",
    "        # Make forecast\n",
    "        print(\"\\nGenerating forecast...\")\n",
    "        forecast_df = make_forecast(model, train_df)\n",
    "        \n",
    "        # Save results\n",
    "        output_dir = \"results\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        forecast_file = os.path.join(output_dir, f\"forecast_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "        forecast_df.to_csv(forecast_file, index=False)\n",
    "        print(f\"\\nForecast saved to {forecast_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting weather forecasting pipeline...\")\n",
    "    main()\n",
    "    print(\"\\nPipeline completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Error during data generation: name 'add_time_features' is not defined\n",
      "\n",
      "ƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...\n",
      "Error during model evaluation: Graph execution error:\n",
      "\n",
      "2 root error(s) found.\n",
      "  (0) UNKNOWN:  NameError: name 'add_time_features' is not defined\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_24376\\3244699436.py\", line 36, in data_generator\n",
      "    chunk = add_time_features(chunk)  # G·ªçi h√†m ƒë·ªÉ th√™m c√°c c·ªôt hour, day, month, season\n",
      "\n",
      "NameError: name 'add_time_features' is not defined\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "\t [[Shape/_4]]\n",
      "  (1) UNKNOWN:  NameError: name 'add_time_features' is not defined\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_24376\\3244699436.py\", line 36, in data_generator\n",
      "    chunk = add_time_features(chunk)  # G·ªçi h√†m ƒë·ªÉ th√™m c√°c c·ªôt hour, day, month, season\n",
      "\n",
      "NameError: name 'add_time_features' is not defined\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_test_function_13247]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi scaler v√† m√¥ h√¨nh ƒë√£ l∆∞u\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "# C√°c c·ªôt ƒë·∫∑c tr∆∞ng v√† m·ª•c ti√™u\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "# Th·ªùi gian chu·ªói\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n t·ªõi file test v√† m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
    "test_file = \"../dataset/real_weather_data.csv\"\n",
    "checkpoint_path = \"best_model_fine_tune.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "# Hi·ªÉn th·ªã m√¥ h√¨nh\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        # Th√™m c√°c c·ªôt th·ªùi gian\n",
    "        chunk = add_time_features(chunk)  # G·ªçi h√†m ƒë·ªÉ th√™m c√°c c·ªôt hour, day, month, season\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o c·ªôt PRECTOTCORR c√≥ m·∫∑t\n",
    "        chunk = reorder_and_clean_columns(chunk)  # ƒê·∫£m b·∫£o PRECTOTCORR ƒë∆∞·ª£c th√™m v√†o\n",
    "        \n",
    "        # Ti·∫øn h√†nh chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "# T·∫°o dataset t·ª´ generator\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),  # S·ª≠a: S·ª≠ d·ª•ng None thay cho batch_size\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),  # S·ª≠a: S·ª≠ d·ª•ng None thay cho batch_size\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Ki·ªÉm tra n·∫øu c√≥ d·ªØ li·ªáu h·ª£p l·ªá\n",
    "try:\n",
    "    for X_batch, y_batch in data_generator(test_file, feature_cols, target_cols, batch_size, timesteps):\n",
    "        print(f\"X_batch shape: {X_batch.shape}, y_batch shape: {y_batch.shape}\")\n",
    "        break  # Ch·ªâ ki·ªÉm tra m·ªôt batch\n",
    "except Exception as e:\n",
    "    print(f\"Error during data generation: {e}\")\n",
    "\n",
    "# ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test\n",
    "try:\n",
    "    print(\"\\nƒêang ƒë√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test...\")\n",
    "    loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "    print(f\"‚úÖ K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n d·ªØ li·ªáu test: Loss={loss:.4f}, MAE={mae:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model evaluation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather_lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
