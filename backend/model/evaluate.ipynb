{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model epoch 4 no embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "30250/30250 [==============================] - 901s 29ms/step - loss: 0.0054 - mae: 0.0264\n",
      "âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u test: Loss=0.0054, MAE=0.0264\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "\n",
    "checkpoint_path = \"../model/best_model2.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "loss, mae = model.evaluate(test_dataset)\n",
    "print(f\"âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u test: Loss={loss:.4f}, MAE={mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune & CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Äang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test...\n",
      "30250/30250 [==============================] - 689s 23ms/step - loss: 0.0049 - mae: 0.0234\n",
      "âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u test: Loss=0.0049, MAE=0.0234\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "checkpoint_path = \"best_model_spatial.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\", \"Latitude\", \"Longitude\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\nÄang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test...\")\n",
    "loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u test: Loss={loss:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Äang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test...\n",
      "30250/30250 [==============================] - 739s 24ms/step - loss: 0.0046 - mae: 0.0223\n",
      "âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u test: Loss=0.0046, MAE=0.0223\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "checkpoint_path = \"best_model_spatial.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\", \"Latitude\", \"Longitude\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\nÄang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test...\")\n",
    "loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u test: Loss={loss:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 8ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 7ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 5ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "17/17 [==============================] - 0s 6ms/step\n",
      "\n",
      "âœ… Sá»‘ lÆ°á»£ng dÃ²ng Ä‘Æ°á»£c so sÃ¡nh: 166464\n",
      "    Datetime  Latitude  Longitude  Predicted_T2M  Predicted_QV2M\n",
      "0 2024-07-26       8.0      102.0     480.932922       68.425316\n",
      "1 2024-07-26       8.0      102.0     480.932922       68.425316\n",
      "2 2024-07-26       8.0      102.0     480.932922       68.425316\n",
      "3 2024-07-26       8.0      102.0     480.932922       68.425316\n",
      "4 2024-07-26       8.0      102.0     480.932922       68.425316\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PRECTOTCORR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PRECTOTCORR'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m forecast_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m forecast_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    144\u001b[0m forecast_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m forecast_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 146\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_day_21\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m forecast_results\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforecast_day26.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ dá»± bÃ¡o vÃ o forecast_day26.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 118\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(df_day_21, forecast_results)\u001b[0m\n\u001b[0;32m    116\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLRSKY_SFC_SW_DWN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT2M\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQV2M\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWS10M\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRECTOTCORR\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# Sá»­a tÃªn cá»™t á»Ÿ Ä‘Ã¢y\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     actual \u001b[38;5;241m=\u001b[39m \u001b[43mmerged\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    119\u001b[0m     pred \u001b[38;5;241m=\u001b[39m merged[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    120\u001b[0m     metrics[col] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_absolute_error(actual, pred),\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: mean_squared_error(actual, pred),\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m: r2_score(actual, pred)\n\u001b[0;32m    124\u001b[0m     }\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'PRECTOTCORR'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========================== LOAD & CLEAN DATA ==========================\n",
    "\n",
    "def load_processed_data():\n",
    "    df_day_20 = pd.read_csv('../dataset/weather_data_25.csv')\n",
    "    df_day_21 = pd.read_csv('../dataset/weather_data_26.csv')\n",
    "    return df_day_20, df_day_21\n",
    "\n",
    "def add_time_features(df):\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')\n",
    "    df['hour'] = df['Datetime'].dt.hour\n",
    "    df['day'] = df['Datetime'].dt.day\n",
    "    df['month'] = df['Datetime'].dt.month\n",
    "    df['season'] = (df['month'] % 12 + 3) // 3\n",
    "    return df\n",
    "\n",
    "# ========================== LSTM INPUT ==========================\n",
    "\n",
    "def create_lstm_input(df, time_steps=24):\n",
    "    X, y = [], []\n",
    "    features = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "\n",
    "    for i in range(len(df) - time_steps - 24 + 1):\n",
    "        X_window = df.iloc[i:i+time_steps][features].values\n",
    "        y_window = df.iloc[i+time_steps:i+time_steps+24][features[:6]].values\n",
    "        if y_window.shape[0] == 24:\n",
    "            X.append(X_window)\n",
    "            y.append(y_window)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ========================== FORECAST ==========================\n",
    "\n",
    "def predict_with_model_for_lat_lon(df_day_20, lat, lon, scaler_X, scaler_y, model):\n",
    "    df_lat_lon = df_day_20[(df_day_20['Latitude'] == lat) & (df_day_20['Longitude'] == lon)].copy()\n",
    "    if len(df_lat_lon) == 0:\n",
    "        print(f\"No data for lat {lat}, lon {lon}\")\n",
    "        return None, None\n",
    "\n",
    "    # ThÃªm cá»™t thá»i gian\n",
    "    df_lat_lon = add_time_features(df_lat_lon)\n",
    "    \n",
    "    # Äáº£m báº£o cá»™t PRECTOTCORR cÃ³ máº·t\n",
    "    df_lat_lon = reorder_and_clean_columns(df_lat_lon)\n",
    "    \n",
    "    X_input, _ = create_lstm_input(df_lat_lon, time_steps=24)\n",
    "    if X_input.shape[0] == 0:\n",
    "        print(f\"Not enough data for prediction at lat {lat}, lon {lon}\")\n",
    "        return None, None\n",
    "\n",
    "    features = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "\n",
    "    X_input_df = pd.DataFrame(X_input.reshape(-1, len(features)), columns=features)\n",
    "    X_input_df = X_input_df[scaler_X.feature_names_in_]\n",
    "    X_input_scaled = scaler_X.transform(X_input_df).reshape(X_input.shape)\n",
    "\n",
    "    y_pred_scaled = model.predict(X_input_scaled)\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled[-1])  # 24 dÃ²ng cuá»‘i\n",
    "\n",
    "    forecast_start = df_lat_lon['Datetime'].iloc[-1] + pd.Timedelta(hours=1)\n",
    "    forecast_times = pd.date_range(start=forecast_start, periods=24, freq='h')\n",
    "\n",
    "    forecast_df = pd.DataFrame(y_pred, columns=[\n",
    "        'Predicted_T2M', 'Predicted_QV2M', 'Predicted_PS', 'Predicted_WS10M',\n",
    "        'Predicted_PRECTOTCORR', 'Predicted_CLRSKY_SFC_SW_DWN'\n",
    "    ])\n",
    "    forecast_df['Datetime'] = forecast_times\n",
    "    forecast_df['Latitude'] = lat\n",
    "    forecast_df['Longitude'] = lon\n",
    "\n",
    "    return forecast_df, y_pred\n",
    "\n",
    "def forecast_for_all_lat_lon(df_day_20, scaler_X, scaler_y):\n",
    "    model = load_model('best_model_fine_tune.h5')\n",
    "    available_coords = df_day_20[['Latitude', 'Longitude']].drop_duplicates().values\n",
    "    forecast_results_list = []\n",
    "\n",
    "    for lat, lon in available_coords:\n",
    "        forecast_df, _ = predict_with_model_for_lat_lon(df_day_20, lat, lon, scaler_X, scaler_y, model)\n",
    "        if forecast_df is not None:\n",
    "            forecast_results_list.append(forecast_df)\n",
    "\n",
    "    forecast_results = pd.concat(forecast_results_list, ignore_index=True)\n",
    "    return forecast_results\n",
    "\n",
    "# ========================== EVALUATE ==========================\n",
    "\n",
    "def evaluate_model(df_day_21, forecast_results):\n",
    "    df_day_21['Datetime'] = pd.to_datetime(df_day_21['Datetime'], errors='coerce')\n",
    "    forecast_results['Datetime'] = pd.to_datetime(forecast_results['Datetime'], errors='coerce')\n",
    "\n",
    "    df_day_21['Latitude'] = df_day_21['Latitude'].astype(np.float32)\n",
    "    df_day_21['Longitude'] = df_day_21['Longitude'].astype(np.float32)\n",
    "    forecast_results['Latitude'] = forecast_results['Latitude'].astype(np.float32)\n",
    "    forecast_results['Longitude'] = forecast_results['Longitude'].astype(np.float32)\n",
    "\n",
    "    merged = pd.merge(df_day_21, forecast_results,\n",
    "                      on=['Datetime', 'Latitude', 'Longitude'],\n",
    "                      how='inner', suffixes=('', '_pred'))\n",
    "\n",
    "    if len(merged) == 0:\n",
    "        print(\"No matching data for evaluation\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nâœ… Sá»‘ lÆ°á»£ng dÃ²ng Ä‘Æ°á»£c so sÃ¡nh: {len(merged)}\")\n",
    "    print(merged[['Datetime', 'Latitude', 'Longitude'] + [f'Predicted_{c}' for c in ['T2M', 'QV2M']]].head())\n",
    "\n",
    "    metrics = {}\n",
    "    for col in [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"]:  # Sá»­a tÃªn cá»™t á»Ÿ Ä‘Ã¢y\n",
    "        actual = merged[col]\n",
    "        pred = merged[f'Predicted_{col}']\n",
    "        metrics[col] = {\n",
    "            'MAE': mean_absolute_error(actual, pred),\n",
    "            'MSE': mean_squared_error(actual, pred),\n",
    "            'R2': r2_score(actual, pred)\n",
    "        }\n",
    "\n",
    "    print(\"\\nğŸ” Chi tiáº¿t cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡:\\n\")\n",
    "    df_metrics = pd.DataFrame(metrics).T\n",
    "    df_metrics.columns = ['MAE', 'MSE', 'R2']\n",
    "    print(df_metrics.round(4))\n",
    "\n",
    "# ========================== MAIN ==========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_day_20, df_day_21 = load_processed_data()\n",
    "    df_day_20_unscaled = df_day_20.copy()\n",
    "\n",
    "    with open('../dataset/scaler.pkl', 'rb') as f:\n",
    "        scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "    forecast_results = forecast_for_all_lat_lon(df_day_20_unscaled, scaler_X, scaler_y)\n",
    "\n",
    "    if len(forecast_results) > 0:\n",
    "        forecast_results['Latitude'] = forecast_results['Latitude'].round(4)\n",
    "        forecast_results['Longitude'] = forecast_results['Longitude'].round(4)\n",
    "\n",
    "        evaluate_model(df_day_21, forecast_results)\n",
    "        \n",
    "        forecast_results.to_csv(\"forecast_day26.csv\", index=False)\n",
    "        print(\"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ dá»± bÃ¡o vÃ o forecast_day26.csv\")\n",
    "\n",
    "        print(\"\\nğŸ“Š Káº¿t quáº£ dá»± bÃ¡o:\")\n",
    "        print(forecast_results.head())\n",
    "        print(\"ğŸ“ˆ Khoáº£ng thá»i gian dá»± bÃ¡o:\",\n",
    "              forecast_results['Datetime'].min(), \"â†’\", forecast_results['Datetime'].max())\n",
    "\n",
    "        print(\"\\nğŸ“Š Dá»¯ liá»‡u thá»±c táº¿:\")\n",
    "        print(df_day_21.head())\n",
    "        print(\"ğŸ“ˆ Khoáº£ng thá»i gian thá»±c táº¿:\",\n",
    "              df_day_21['Datetime'].min(), \"â†’\", df_day_21['Datetime'].max())\n",
    "    else:\n",
    "        print(\"âŒ KhÃ´ng cÃ³ káº¿t quáº£ dá»± bÃ¡o Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.\")\n",
    "        \n",
    "            # Hiá»ƒn thá»‹ káº¿t quáº£ dá»± bÃ¡o vÃ  thá»±c táº¿ chi tiáº¿t hÆ¡n\n",
    "        print(\"\\nğŸ“‹ Má»™t vÃ i dÃ²ng dá»¯ liá»‡u dá»± bÃ¡o:\")\n",
    "        print(forecast_results[['Datetime', 'Latitude', 'Longitude'] + [col for col in forecast_results.columns if col.startswith('Predicted_')]].head(10))\n",
    "\n",
    "        print(\"\\nğŸ“‹ Má»™t vÃ i dÃ²ng dá»¯ liá»‡u thá»±c táº¿:\")\n",
    "        print(df_day_21[['Datetime', 'Latitude', 'Longitude', 'T2M', 'QV2M', 'PS', 'WS10M', 'PRECTOTCORR', 'CLRSKY_SFC_SW_DWN']].head(10))\n",
    "\n",
    "    # Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh\n",
    "    def plot_results_for_location(df_actual, df_pred, lat, lon):\n",
    "        actual_data = df_actual[(df_actual['Latitude'] == lat) & (df_actual['Longitude'] == lon)]\n",
    "        pred_data = df_pred[(df_pred['Latitude'] == lat) & (df_pred['Longitude'] == lon)]\n",
    "\n",
    "        if actual_data.empty or pred_data.empty:\n",
    "            print(f\"KhÃ´ng cÃ³ dá»¯ liá»‡u cho toáº¡ Ä‘á»™ ({lat}, {lon})\")\n",
    "            return\n",
    "\n",
    "        cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"]\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        for i, col in enumerate(cols):\n",
    "            plt.subplot(3, 2, i+1)\n",
    "            plt.plot(actual_data['Datetime'], actual_data[col], label='Thá»±c táº¿', color='blue')\n",
    "            plt.plot(pred_data['Datetime'], pred_data[f'Predicted_{col}'], label='Dá»± bÃ¡o', linestyle='--', color='red')\n",
    "            plt.title(f'{col} táº¡i ({lat}, {lon})')\n",
    "            plt.xlabel('Thá»i gian')\n",
    "            plt.ylabel(col)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Chá»n má»™t toáº¡ Ä‘á»™ phá»• biáº¿n nháº¥t Ä‘á»ƒ váº½\n",
    "    most_common_coord = forecast_results.groupby(['Latitude', 'Longitude']).size().idxmax()\n",
    "    plot_results_for_location(df_day_21, forecast_results, *most_common_coord)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG SCALERS ===\n",
      "Scaler_X details:\n",
      "- Feature names: ['Latitude' 'Longitude' 'hour' 'day' 'month' 'season' 'WS10M' 'QV2M' 'PS'\n",
      " 'PRECTOTCORR' 'T2M' 'CLRSKY_SFC_SW_DWN']\n",
      "- Data min: [   8.  102.    0.    1.    1.    1. -999. -999. -999.    0. -999. -999.]\n",
      "- Data max: [  24.    118.     23.     31.     12.      4.   1072.65 1063.78 1031.07\n",
      " 2178.57 1087.62 1073.12]\n",
      "- Scale: [0.0625     0.0625     0.04347826 0.03333334 0.09090909 0.33333334\n",
      " 0.00048271 0.00048478 0.00049259 0.00045902 0.00047924 0.0004826 ]\n",
      "- Min: [-0.5        -6.375       0.         -0.03333334 -0.09090909 -0.33333334\n",
      "  0.48222435  0.4842979   0.49210125  0.          0.47876468  0.4821149 ]\n",
      "\n",
      "Scaler_y details:\n",
      "- Feature names: ['CLRSKY_SFC_SW_DWN' 'PS' 'T2M' 'QV2M' 'WS10M' 'PRECTOTCORR']\n",
      "- Data min: [-999. -999. -999. -999. -999.    0.]\n",
      "- Data max: [1073.12 1031.07 1087.62 1063.78 1072.65 2178.57]\n",
      "- Scale: [0.0004826  0.00049259 0.00047924 0.00048478 0.00048271 0.00045902]\n",
      "- Min: [0.4821149  0.49210125 0.47876468 0.4842979  0.48222435 0.        ]\n",
      "\n",
      "Checking training data...\n",
      "\n",
      "=== DEBUG TRAINING DATA SAMPLE ===\n",
      "Shape: (24, 13)\n",
      "\n",
      "First 3 rows:\n",
      "             Datetime  Latitude  Longitude    T2M   QV2M    PS   WS10M  \\\n",
      "0 2024-07-25 00:00:00         8        102  30.14  22.61  3.38  100.63   \n",
      "1 2024-07-25 01:00:00         8        102  30.08  22.50  4.15  100.56   \n",
      "2 2024-07-25 02:00:00         8        102  30.12  21.71  4.90  100.50   \n",
      "\n",
      "   CLRSKY_SFC_SW_DWN  PRECTOTCORR  hour  day  month  season  \n",
      "0                0.0         5.50     0   25      7       3  \n",
      "1                0.0         5.37     1   25      7       3  \n",
      "2                0.0         4.57     2   25      7       3  \n",
      "\n",
      "Last 3 rows:\n",
      "              Datetime  Latitude  Longitude    T2M   QV2M    PS   WS10M  \\\n",
      "21 2024-07-25 21:00:00         8        102  30.79  19.97  3.73  100.72   \n",
      "22 2024-07-25 22:00:00         8        102  30.72  19.94  3.04  100.73   \n",
      "23 2024-07-25 23:00:00         8        102  30.63  19.75  2.28  100.69   \n",
      "\n",
      "    CLRSKY_SFC_SW_DWN  PRECTOTCORR  hour  day  month  season  \n",
      "21                0.0         5.52    21   25      7       3  \n",
      "22                0.0         6.72    22   25      7       3  \n",
      "23                0.0        10.88    23   25      7       3  \n",
      "\n",
      "Descriptive stats:\n",
      "                  Datetime  Latitude  Longitude        T2M       QV2M  \\\n",
      "count                   24      24.0       24.0  24.000000  24.000000   \n",
      "mean   2024-07-25 11:30:00       8.0      102.0  30.343333  20.617917   \n",
      "min    2024-07-25 00:00:00       8.0      102.0  29.900000  18.210000   \n",
      "25%    2024-07-25 05:45:00       8.0      102.0  29.975000  19.750000   \n",
      "50%    2024-07-25 11:30:00       8.0      102.0  30.130000  20.930000   \n",
      "75%    2024-07-25 17:15:00       8.0      102.0  30.800000  21.470000   \n",
      "max    2024-07-25 23:00:00       8.0      102.0  30.960000  22.610000   \n",
      "std                    NaN       0.0        0.0   0.414295   1.236981   \n",
      "\n",
      "              PS       WS10M  CLRSKY_SFC_SW_DWN  PRECTOTCORR       hour   day  \\\n",
      "count  24.000000   24.000000          24.000000    24.000000  24.000000  24.0   \n",
      "mean    5.129167  100.600833         280.651667     6.823333  11.500000  25.0   \n",
      "min     2.280000  100.470000           0.000000     1.650000   0.000000  25.0   \n",
      "25%     4.347500  100.500000           0.000000     5.170000   5.750000  25.0   \n",
      "50%     4.950000  100.610000          26.200000     6.220000  11.500000  25.0   \n",
      "75%     5.752500  100.690000         595.837500     8.915000  17.250000  25.0   \n",
      "max     7.870000  100.730000         921.670000    13.690000  23.000000  25.0   \n",
      "std     1.431147    0.092592         357.078331     3.101060   7.071068   0.0   \n",
      "\n",
      "       month  season  \n",
      "count   24.0    24.0  \n",
      "mean     7.0     3.0  \n",
      "min      7.0     3.0  \n",
      "25%      7.0     3.0  \n",
      "50%      7.0     3.0  \n",
      "75%      7.0     3.0  \n",
      "max      7.0     3.0  \n",
      "std      0.0     0.0  \n",
      "\n",
      "Missing values:\n",
      "Datetime             0\n",
      "Latitude             0\n",
      "Longitude            0\n",
      "T2M                  0\n",
      "QV2M                 0\n",
      "PS                   0\n",
      "WS10M                0\n",
      "CLRSKY_SFC_SW_DWN    0\n",
      "PRECTOTCORR          0\n",
      "hour                 0\n",
      "day                  0\n",
      "month                0\n",
      "season               0\n",
      "dtype: int64\n",
      "\n",
      "Checking test data...\n",
      "\n",
      "=== DEBUG TEST DATA SAMPLE ===\n",
      "Shape: (24, 13)\n",
      "\n",
      "First 3 rows:\n",
      "             Datetime  Latitude  Longitude    T2M   QV2M    PS   WS10M  \\\n",
      "0 2024-07-26 00:00:00         8        102  30.54  19.59  1.80  100.65   \n",
      "1 2024-07-26 01:00:00         8        102  30.44  19.72  1.88  100.60   \n",
      "2 2024-07-26 02:00:00         8        102  30.33  20.07  2.32  100.57   \n",
      "\n",
      "   CLRSKY_SFC_SW_DWN  PRECTOTCORR  hour  day  month  season  \n",
      "0                0.0        13.31     0   26      7       3  \n",
      "1                0.0        11.40     1   26      7       3  \n",
      "2                0.0         4.59     2   26      7       3  \n",
      "\n",
      "Last 3 rows:\n",
      "              Datetime  Latitude  Longitude    T2M   QV2M    PS   WS10M  \\\n",
      "21 2024-07-26 21:00:00         8        102  30.05  17.32  1.85  100.85   \n",
      "22 2024-07-26 22:00:00         8        102  30.10  17.21  1.40  100.86   \n",
      "23 2024-07-26 23:00:00         8        102  30.09  17.41  1.63  100.83   \n",
      "\n",
      "    CLRSKY_SFC_SW_DWN  PRECTOTCORR  hour  day  month  season  \n",
      "21                0.0         2.70    21   26      7       3  \n",
      "22                0.0         2.50    22   26      7       3  \n",
      "23                0.0         2.37    23   26      7       3  \n",
      "\n",
      "Descriptive stats:\n",
      "                  Datetime  Latitude  Longitude        T2M      QV2M  \\\n",
      "count                   24      24.0       24.0  24.000000  24.00000   \n",
      "mean   2024-07-26 11:30:00       8.0      102.0  30.082083  19.74875   \n",
      "min    2024-07-26 00:00:00       8.0      102.0  29.540000  17.21000   \n",
      "25%    2024-07-26 05:45:00       8.0      102.0  29.905000  19.37500   \n",
      "50%    2024-07-26 11:30:00       8.0      102.0  30.150000  19.91500   \n",
      "75%    2024-07-26 17:15:00       8.0      102.0  30.315000  20.81250   \n",
      "max    2024-07-26 23:00:00       8.0      102.0  30.540000  21.21000   \n",
      "std                    NaN       0.0        0.0   0.290247   1.28865   \n",
      "\n",
      "             PS       WS10M  CLRSKY_SFC_SW_DWN  PRECTOTCORR       hour   day  \\\n",
      "count  24.00000   24.000000          24.000000    24.000000  24.000000  24.0   \n",
      "mean    3.64250  100.723750         283.579167     6.404167  11.500000  26.0   \n",
      "min     1.40000  100.570000           0.000000     0.130000   0.000000  26.0   \n",
      "25%     2.62000  100.637500           0.000000     0.760000   5.750000  26.0   \n",
      "50%     3.76000  100.735000          27.300000     3.645000  11.500000  26.0   \n",
      "75%     4.54750  100.830000         600.447500    11.877500  17.250000  26.0   \n",
      "max     5.72000  100.860000         930.950000    18.800000  23.000000  26.0   \n",
      "std     1.31888    0.101545         360.806920     6.474358   7.071068   0.0   \n",
      "\n",
      "       month  season  \n",
      "count   24.0    24.0  \n",
      "mean     7.0     3.0  \n",
      "min      7.0     3.0  \n",
      "25%      7.0     3.0  \n",
      "50%      7.0     3.0  \n",
      "75%      7.0     3.0  \n",
      "max      7.0     3.0  \n",
      "std      0.0     0.0  \n",
      "\n",
      "Missing values:\n",
      "Datetime             0\n",
      "Latitude             0\n",
      "Longitude            0\n",
      "T2M                  0\n",
      "QV2M                 0\n",
      "PS                   0\n",
      "WS10M                0\n",
      "CLRSKY_SFC_SW_DWN    0\n",
      "PRECTOTCORR          0\n",
      "hour                 0\n",
      "day                  0\n",
      "month                0\n",
      "season               0\n",
      "dtype: int64\n",
      "\n",
      "Evaluating model...\n",
      "Warning: Chunk size 24 too small for timesteps=24 and forecast=24. Skipping...\n",
      "\n",
      "âŒ Main Error: cannot convert float infinity to integer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\keras\\utils\\generic_utils.py:993: RuntimeWarning: divide by zero encountered in log10\n",
      "  numdigits = int(np.log10(self.target)) + 1\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_24376\\458602434.py\", line 263, in main\n",
      "    evaluate_model()\n",
      "  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_24376\\458602434.py\", line 171, in evaluate_model\n",
      "    loss, mae = model.evaluate(test_dataset, verbose=1)\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\keras\\utils\\generic_utils.py\", line 993, in update\n",
      "    numdigits = int(np.log10(self.target)) + 1\n",
      "OverflowError: cannot convert float infinity to integer\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "train_file = \"../dataset/weather_data_25.csv\"  # DÃ¹ng Ä‘á»ƒ dá»± bÃ¡o\n",
    "test_file = \"../dataset/weather_data_26.csv\"   # DÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡\n",
    "checkpoint_path = \"best_model_fine_tune.h5\"\n",
    "\n",
    "# 2. CÃ¡c cá»™t Ä‘áº·c trÆ°ng vÃ  má»¥c tiÃªu\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "               \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"]\n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "# 3. Táº£i model vÃ  scaler\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "# 4. HÃ m tiá»n xá»­ lÃ½ dá»¯ liá»‡u (cáº£i tiáº¿n)\n",
    "def preprocess_data(df):\n",
    "    # Chuáº©n hÃ³a tÃªn cá»™t\n",
    "    if 'PRECTOT' in df.columns and 'PRECTOTCORR' not in df.columns:\n",
    "        df = df.rename(columns={'PRECTOT': 'PRECTOTCORR'})\n",
    "    \n",
    "    # ThÃªm cÃ¡c Ä‘áº·c trÆ°ng thá»i gian\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df['hour'] = df['Datetime'].dt.hour.astype(np.int8)\n",
    "    df['day'] = df['Datetime'].dt.day.astype(np.int8)\n",
    "    df['month'] = df['Datetime'].dt.month.astype(np.int8)\n",
    "    df['season'] = ((df['month'] % 12 + 3) // 3).astype(np.int8)\n",
    "    \n",
    "    # Xá»­ lÃ½ dá»¯ liá»‡u sá»‘\n",
    "    for col in feature_cols + target_cols:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = pd.to_numeric(df[col].astype(str).str.extract('([-+]?\\d*\\.?\\d+)')[0], errors='coerce')\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "    return df\n",
    "\n",
    "def evaluation_data_generator(file_path):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size*5, dtype=dtype_dict, parse_dates=[\"Datetime\"]):\n",
    "        chunk = preprocess_data(chunk)\n",
    "        \n",
    "        # Kiá»ƒm tra náº¿u chunk Ä‘á»§ dÃ i\n",
    "        if len(chunk) < timesteps + 24:\n",
    "            print(f\"Warning: Chunk size {len(chunk)} too small for timesteps={timesteps} and forecast=24. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "        \n",
    "        if X_batch:  # Chá»‰ yield náº¿u cÃ³ dá»¯ liá»‡u\n",
    "            yield np.array(X_batch), np.array(y_batch)\n",
    "        else:\n",
    "            print(\"Warning: No valid sequences generated from this chunk.\")\n",
    "\n",
    "def debug_scalers(scaler_X, scaler_y):\n",
    "    print(\"\\n=== DEBUG SCALERS ===\")\n",
    "    print(\"Scaler_X details:\")\n",
    "    print(f\"- Feature names: {scaler_X.feature_names_in_}\")\n",
    "    print(f\"- Data min: {scaler_X.data_min_}\")\n",
    "    print(f\"- Data max: {scaler_X.data_max_}\")\n",
    "    print(f\"- Scale: {scaler_X.scale_}\")\n",
    "    print(f\"- Min: {scaler_X.min_}\")\n",
    "    \n",
    "    print(\"\\nScaler_y details:\")\n",
    "    print(f\"- Feature names: {scaler_y.feature_names_in_}\")\n",
    "    print(f\"- Data min: {scaler_y.data_min_}\")\n",
    "    print(f\"- Data max: {scaler_y.data_max_}\")\n",
    "    print(f\"- Scale: {scaler_y.scale_}\")\n",
    "    print(f\"- Min: {scaler_y.min_}\")\n",
    "\n",
    "def debug_data_sample(df, name=\"Data\"):\n",
    "    print(f\"\\n=== DEBUG {name.upper()} SAMPLE ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 3 rows:\")\n",
    "    print(df.head(3))\n",
    "    print(\"\\nLast 3 rows:\")\n",
    "    print(df.tail(3))\n",
    "    print(\"\\nDescriptive stats:\")\n",
    "    print(df.describe())\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "def debug_model_predictions(y_true, y_pred, target_names):\n",
    "    print(\"\\n=== DEBUG PREDICTIONS ===\")\n",
    "    for i, col in enumerate(target_names):\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"- True mean: {y_true[:,i].mean():.4f}\")\n",
    "        print(f\"- Pred mean: {y_pred[:,i].mean():.4f}\")\n",
    "        print(f\"- MAE: {np.mean(np.abs(y_true[:,i] - y_pred[:,i])):.4f}\")\n",
    "        print(f\"- Max Error: {np.max(np.abs(y_true[:,i] - y_pred[:,i])):.4f}\")\n",
    "        \n",
    "# 6. HÃ m dá»± bÃ¡o (tá»« cÃ¡ch thá»© 1 nhÆ°ng Ä‘Ã£ cáº£i tiáº¿n)\n",
    "def make_forecast(model, input_data):\n",
    "    print(\"\\n=== FORECAST DEBUG START ===\")\n",
    "    \n",
    "    # Tiá»n xá»­ lÃ½\n",
    "    input_data = preprocess_data(input_data)\n",
    "    debug_data_sample(input_data[feature_cols + target_cols], \"Input Data\")\n",
    "    \n",
    "    # Láº¥y dá»¯ liá»‡u cuá»‘i cÃ¹ng\n",
    "    X_df = input_data[feature_cols].iloc[-timesteps:]\n",
    "    print(f\"\\nUsing last {timesteps} timesteps for prediction:\")\n",
    "    print(X_df)\n",
    "    \n",
    "    # Chuáº©n hÃ³a\n",
    "    print(\"\\nScaling details:\")\n",
    "    print(\"First row before scaling:\", X_df.iloc[0].values)\n",
    "    X_scaled = scaler_X.transform(X_df)\n",
    "    print(\"First row after scaling:\", X_scaled[0])\n",
    "    \n",
    "    X_scaled = X_scaled.reshape(1, timesteps, len(feature_cols))\n",
    "    \n",
    "    # Dá»± bÃ¡o\n",
    "    print(\"\\nMaking prediction...\")\n",
    "    y_pred_scaled = model.predict(X_scaled, verbose=1)\n",
    "    print(\"\\nRaw scaled predictions:\")\n",
    "    print(y_pred_scaled)\n",
    "    \n",
    "    # Inverse transform\n",
    "    print(\"\\nInverse transforming predictions...\")\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, len(target_cols)))\n",
    "    print(\"First 5 predicted values after inverse transform:\")\n",
    "    print(y_pred[:5])\n",
    "    \n",
    "    # Táº¡o DataFrame káº¿t quáº£\n",
    "    forecast_dates = pd.date_range(\n",
    "        start=input_data['Datetime'].iloc[-1] + pd.Timedelta(hours=1),\n",
    "        periods=24,\n",
    "        freq='h'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== FORECAST DEBUG END ===\")\n",
    "    return pd.DataFrame({\n",
    "        'Datetime': forecast_dates,\n",
    "        **{f'Predicted_{col}': y_pred[:, i] for i, col in enumerate(target_cols)}\n",
    "    })\n",
    "\n",
    "# 7. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
    "def evaluate_model():\n",
    "    # Táº¡o dataset\n",
    "    test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: evaluation_data_generator(test_file),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32)\n",
    "        )\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # ÄÃ¡nh giÃ¡\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "    print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
    "\n",
    "    # Dá»± bÃ¡o máº«u Ä‘á»ƒ debug\n",
    "    print(\"\\nRunning sample predictions for debugging...\")\n",
    "    for X_batch, y_batch in evaluation_data_generator(test_file):\n",
    "        # Láº¥y batch Ä‘áº§u tiÃªn\n",
    "        X_sample = X_batch[0:1]  # Láº¥y máº«u Ä‘áº§u tiÃªn\n",
    "        y_true = y_batch[0:1]   # GiÃ¡ trá»‹ thá»±c táº¿ tÆ°Æ¡ng á»©ng\n",
    "        \n",
    "        # Dá»± bÃ¡o\n",
    "        y_pred_scaled = model.predict(X_sample, verbose=0)\n",
    "        \n",
    "        # Inverse transform\n",
    "        y_true = scaler_y.inverse_transform(y_true.reshape(-1, len(target_cols)))\n",
    "        y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, len(target_cols)))\n",
    "        \n",
    "        # Debug\n",
    "        debug_model_predictions(y_true, y_pred, target_cols)\n",
    "        break  # Chá»‰ kiá»ƒm tra batch Ä‘áº§u tiÃªn\n",
    "\n",
    "# 8. So sÃ¡nh dá»± bÃ¡o vá»›i thá»±c táº¿\n",
    "def compare_results(forecast_df):\n",
    "    # Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u thá»±c táº¿\n",
    "    actual_df = preprocess_data(pd.read_csv(test_file))\n",
    "    actual_24h = actual_df.iloc[:24].copy()\n",
    "    \n",
    "    # GhÃ©p dá»¯ liá»‡u\n",
    "    comparison = pd.merge(\n",
    "        forecast_df.rename(columns={f'Predicted_{col}': col for col in target_cols}),\n",
    "        actual_24h[['Datetime'] + target_cols],\n",
    "        on='Datetime',\n",
    "        suffixes=('_pred', '_actual')\n",
    "    )\n",
    "    \n",
    "    # TÃ­nh toÃ¡n sai sá»‘\n",
    "    for col in target_cols:\n",
    "        comparison[f'{col}_error'] = comparison[f'{col}_actual'] - comparison[f'{col}_pred']\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, col in enumerate(target_cols, 1):\n",
    "        plt.subplot(3, 2, i)\n",
    "        plt.plot(comparison['Datetime'], comparison[f'{col}_actual'], label='Thá»±c táº¿')\n",
    "        plt.plot(comparison['Datetime'], comparison[f'{col}_pred'], label='Dá»± bÃ¡o')\n",
    "        plt.title(col)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# HÃ m check_data_distribution sá»­a láº¡i\n",
    "def check_data_distribution():\n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u trÆ°á»›c khi kiá»ƒm tra\n",
    "    train_df = preprocess_data(train_df)\n",
    "    test_df = preprocess_data(test_df)\n",
    "    \n",
    "    print(\"\\nPhÃ¢n phá»‘i dá»¯ liá»‡u train vs test:\")\n",
    "    for col in feature_cols + target_cols:\n",
    "        if col in train_df.columns and col in test_df.columns:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(train_df[col], bins=30, alpha=0.5, label='Train')\n",
    "            plt.hist(test_df[col], bins=30, alpha=0.5, label='Test')\n",
    "            plt.title(col)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Warning: Column {col} not found in data\")\n",
    "\n",
    "# HÃ m main sá»­a láº¡i\n",
    "def main():\n",
    "    try:\n",
    "        # Kiá»ƒm tra scaler chi tiáº¿t\n",
    "        debug_scalers(scaler_X, scaler_y)\n",
    "        \n",
    "        # Kiá»ƒm tra dá»¯ liá»‡u train/test\n",
    "        print(\"\\nChecking training data...\")\n",
    "        train_df = pd.read_csv(train_file)\n",
    "        train_df = preprocess_data(train_df)\n",
    "        debug_data_sample(train_df, \"Training Data\")\n",
    "        \n",
    "        print(\"\\nChecking test data...\")\n",
    "        test_df = pd.read_csv(test_file)\n",
    "        test_df = preprocess_data(test_df)\n",
    "        debug_data_sample(test_df, \"Test Data\")\n",
    "        \n",
    "        # ÄÃ¡nh giÃ¡ model vá»›i debug\n",
    "        evaluate_model()\n",
    "        \n",
    "        # Dá»± bÃ¡o vá»›i debug chi tiáº¿t\n",
    "        print(\"\\nMaking forecast with debug...\")\n",
    "        forecast_df = make_forecast(model, train_df)\n",
    "        \n",
    "        if forecast_df is not None:\n",
    "            print(\"\\nForecast results:\")\n",
    "            print(forecast_df)\n",
    "            \n",
    "            # So sÃ¡nh vá»›i thá»±c táº¿\n",
    "            comparison_df = compare_results(forecast_df)\n",
    "            \n",
    "            # TÃ­nh toÃ¡n sai sá»‘ chi tiáº¿t\n",
    "            print(\"\\nError Analysis:\")\n",
    "            for col in target_cols:\n",
    "                if f'{col}_error' in comparison_df.columns:\n",
    "                    errors = comparison_df[f'{col}_error']\n",
    "                    print(f\"\\n{col}:\")\n",
    "                    print(f\"- Mean: {errors.mean():.4f}\")\n",
    "                    print(f\"- MAE: {errors.abs().mean():.4f}\")\n",
    "                    print(f\"- Max Error: {errors.abs().max():.4f}\")\n",
    "                    print(f\"- Std Dev: {errors.std():.4f}\")\n",
    "                    print(f\"- Median: {errors.median():.4f}\")\n",
    "                    \n",
    "                    # PhÃ¢n phá»‘i sai sá»‘\n",
    "                    plt.figure()\n",
    "                    plt.hist(errors, bins=30)\n",
    "                    plt.title(f'Error Distribution - {col}')\n",
    "                    plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Main Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Kiá»ƒm tra data generator...\n",
      "X_batch shape: (0,), y_batch shape: (0,)\n",
      "Lá»—i khi táº¡o dá»¯ liá»‡u: too many indices for array: array is 1-dimensional, but 3 were indexed\n",
      "\n",
      "âŒ Lá»—i chÃ­nh: too many indices for array: array is 1-dimensional, but 3 were indexed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_24060\\2475724206.py\", line 95, in <module>\n",
      "    print(\"Sample X values:\", X_batch[0, 0, :5])  # In 5 giÃ¡ trá»‹ Ä‘áº§u tiÃªn cá»§a máº«u Ä‘áº§u tiÃªn\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 3 were indexed\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# ÄÆ°á»ng dáº«n tá»›i scaler vÃ  mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "# CÃ¡c cá»™t Ä‘áº·c trÆ°ng vÃ  má»¥c tiÃªu\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "# Thá»i gian chuá»—i\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "# ÄÆ°á»ng dáº«n tá»›i file test vÃ  mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n\n",
    "test_file = \"../dataset/weather_data_26.csv\"\n",
    "checkpoint_path = \"best_model_fine_tune.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "# Hiá»ƒn thá»‹ mÃ´ hÃ¬nh\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        # ThÃªm cÃ¡c cá»™t thá»i gian\n",
    "        chunk = add_time_features(chunk)  # Gá»i hÃ m Ä‘á»ƒ thÃªm cÃ¡c cá»™t hour, day, month, season\n",
    "        \n",
    "        # Äáº£m báº£o cá»™t PRECTOTCORR cÃ³ máº·t\n",
    "        chunk = reorder_and_clean_columns(chunk)  # Äáº£m báº£o PRECTOTCORR Ä‘Æ°á»£c thÃªm vÃ o\n",
    "        \n",
    "        # Tiáº¿n hÃ nh chuáº©n hÃ³a dá»¯ liá»‡u\n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "# Táº¡o dataset tá»« generator\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),  # Sá»­a: Sá»­ dá»¥ng None thay cho batch_size\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),  # Sá»­a: Sá»­ dá»¥ng None thay cho batch_size\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Kiá»ƒm tra náº¿u cÃ³ dá»¯ liá»‡u há»£p lá»‡\n",
    "try:\n",
    "    for X_batch, y_batch in data_generator(test_file, feature_cols, target_cols, batch_size, timesteps):\n",
    "        print(f\"X_batch shape: {X_batch.shape}, y_batch shape: {y_batch.shape}\")\n",
    "        break  # Chá»‰ kiá»ƒm tra má»™t batch\n",
    "except Exception as e:\n",
    "    print(f\"Error during data generation: {e}\")\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test\n",
    "try:\n",
    "    print(\"\\nÄang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test...\")\n",
    "    loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "    print(f\"âœ… Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ trÃªn dá»¯ liá»‡u test: Loss={loss:.4f}, MAE={mae:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model evaluation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather_lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
