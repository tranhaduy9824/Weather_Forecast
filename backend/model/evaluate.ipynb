{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model epoch 4 no embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "\n",
    "checkpoint_path = \"../model/best_model2.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "loss, mae = model.evaluate(test_dataset)\n",
    "print(f\"✅ Kết quả đánh giá trên dữ liệu test: Loss={loss:.4f}, MAE={mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune & CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Đang đánh giá mô hình trên tập test...\n",
      "30250/30250 [==============================] - 689s 23ms/step - loss: 0.0049 - mae: 0.0234\n",
      "✅ Kết quả đánh giá trên dữ liệu test: Loss=0.0049, MAE=0.0234\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "checkpoint_path = \"best_model_spatial.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\", \"Latitude\", \"Longitude\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\nĐang đánh giá mô hình trên tập test...\")\n",
    "loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"✅ Kết quả đánh giá trên dữ liệu test: Loss={loss:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Đang đánh giá mô hình trên tập test...\n",
      "30250/30250 [==============================] - 739s 24ms/step - loss: 0.0046 - mae: 0.0223\n",
      "✅ Kết quả đánh giá trên dữ liệu test: Loss=0.0046, MAE=0.0223\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 256\n",
    "\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "checkpoint_path = \"best_model_spatial.h5\"\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=256, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\", \"Latitude\", \"Longitude\"])\n",
    "        \n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])\n",
    "\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_file, feature_cols, target_cols, batch_size, timesteps),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32), \n",
    "        tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32),\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"\\nĐang đánh giá mô hình trên tập test...\")\n",
    "loss, mae = model.evaluate(test_dataset, verbose=1)\n",
    "print(f\"✅ Kết quả đánh giá trên dữ liệu test: Loss={loss:.4f}, MAE={mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 24, 128)           72192     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 144)               9360      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 24, 6)             0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 130,960\n",
      "Trainable params: 130,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: argument of type 'method' is not iterable\nTraceback (most recent call last):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_21592\\715416960.py\", line 37, in data_generator\n    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n    self.handles = get_handle(\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 719, in get_handle\n    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 1181, in _is_binary_mode\n    return isinstance(handle, _get_binary_io_classes()) or \"b\" in getattr(\n\nTypeError: argument of type 'method' is not iterable\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[Shape/_4]]\n  (1) INVALID_ARGUMENT:  TypeError: argument of type 'method' is not iterable\nTraceback (most recent call last):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_21592\\715416960.py\", line 37, in data_generator\n    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n    self.handles = get_handle(\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 719, in get_handle\n    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 1181, in _is_binary_mode\n    return isinstance(handle, _get_binary_io_classes()) or \"b\" in getattr(\n\nTypeError: argument of type 'method' is not iterable\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_2164]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 77\u001b[0m\n\u001b[0;32m     70\u001b[0m test_dataset_subset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_generator(\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: data_generator(test_subset, feature_cols, target_cols, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m),\n\u001b[0;32m     72\u001b[0m     output_signature\u001b[38;5;241m=\u001b[39m(tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, timesteps, \u001b[38;5;28mlen\u001b[39m(feature_cols)), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32), \n\u001b[0;32m     73\u001b[0m                       tf\u001b[38;5;241m.\u001b[39mTensorSpec(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;28mlen\u001b[39m(target_cols)), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     74\u001b[0m )\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Đánh giá mô hình trên tập test con (subset)\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m loss, mae \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Kết quả đánh giá trên dữ liệu test con (chunk): Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Tính lỗi cho từng mẫu trong tập kiểm tra con\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: argument of type 'method' is not iterable\nTraceback (most recent call last):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_21592\\715416960.py\", line 37, in data_generator\n    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n    self.handles = get_handle(\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 719, in get_handle\n    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 1181, in _is_binary_mode\n    return isinstance(handle, _get_binary_io_classes()) or \"b\" in getattr(\n\nTypeError: argument of type 'method' is not iterable\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[Shape/_4]]\n  (1) INVALID_ARGUMENT:  TypeError: argument of type 'method' is not iterable\nTraceback (most recent call last):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1035, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_21592\\715416960.py\", line 37, in data_generator\n    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n    return _read(filepath_or_buffer, kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n    self._engine = self._make_engine(f, self.engine)\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n    self.handles = get_handle(\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 719, in get_handle\n    if _is_binary_mode(path_or_buf, mode) and \"b\" not in mode:\n\n  File \"d:\\anaconda3\\envs\\weather_lstm\\lib\\site-packages\\pandas\\io\\common.py\", line 1181, in _is_binary_mode\n    return isinstance(handle, _get_binary_io_classes()) or \"b\" in getattr(\n\nTypeError: argument of type 'method' is not iterable\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_test_function_2164]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import gc\n",
    "\n",
    "# Load scaler\n",
    "scaler_file = \"../dataset/processed/scaler.pkl\"\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler_X, scaler_y = pickle.load(f)\n",
    "\n",
    "# Định nghĩa các cột đầu vào và đầu ra\n",
    "feature_cols = [\"Latitude\", \"Longitude\", \"hour\", \"day\", \"month\", \"season\", \n",
    "                \"WS10M\", \"QV2M\", \"PS\", \"PRECTOTCORR\", \"T2M\", \"CLRSKY_SFC_SW_DWN\"]\n",
    "target_cols = [\"CLRSKY_SFC_SW_DWN\", \"PS\", \"T2M\", \"QV2M\", \"WS10M\", \"PRECTOTCORR\"] \n",
    "\n",
    "timesteps = 24\n",
    "batch_size = 64  # Giảm kích thước batch để tránh tràn bộ nhớ\n",
    "\n",
    "# Tải mô hình đã huấn luyện trước\n",
    "checkpoint_path = \"best_model_spatial.h5\"  # Đảm bảo đường dẫn đúng\n",
    "model = load_model(checkpoint_path)\n",
    "\n",
    "# Kiểm tra mô hình\n",
    "print(\"Model: \\\"sequential\\\"\")\n",
    "model.summary()\n",
    "\n",
    "# Định nghĩa data generator để chuẩn bị dữ liệu cho mô hình\n",
    "def data_generator(file_path, feature_cols, target_cols, batch_size=64, timesteps=24):\n",
    "    dtype_dict = {col: np.float32 for col in feature_cols + target_cols}\n",
    "    dtype_dict.update({\"hour\": np.int8, \"day\": np.int8, \"month\": np.int8, \"season\": np.int8})\n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size * 5, dtype=dtype_dict, parse_dates=[\"Datetime\"], low_memory=True):\n",
    "        chunk = chunk.sort_values(by=[\"Datetime\", \"Latitude\", \"Longitude\"])\n",
    "        \n",
    "        # Chuẩn hóa dữ liệu\n",
    "        X_scaled = scaler_X.transform(chunk[feature_cols])\n",
    "        y_scaled = scaler_y.transform(chunk[target_cols])\n",
    "\n",
    "        X_batch, y_batch = [], []\n",
    "        for i in range(len(chunk) - timesteps - 24):\n",
    "            X_batch.append(X_scaled[i:i+timesteps])  # Lưu batch dữ liệu đầu vào\n",
    "            y_batch.append(y_scaled[i+timesteps:i+timesteps+24])  # Lưu batch dữ liệu đầu ra\n",
    "\n",
    "        # Trả về batch dưới dạng np.array với đúng shape\n",
    "        yield np.array(X_batch), np.array(y_batch)\n",
    "\n",
    "# Đường dẫn đến file dữ liệu test\n",
    "test_file = \"../dataset/processed/test_data.csv\"\n",
    "\n",
    "# Đọc tệp dữ liệu theo chunk và lấy 20% cuối từ mỗi chunk\n",
    "chunk_size = 10000  # Bạn có thể điều chỉnh kích thước của chunk\n",
    "test_data_chunks = pd.read_csv(test_file, chunksize=chunk_size)\n",
    "\n",
    "# Khởi tạo các biến để lưu trữ kết quả\n",
    "y_true = []\n",
    "y_pred = []\n",
    "errors = []\n",
    "\n",
    "# Lặp qua từng chunk để xử lý và đánh giá mô hình\n",
    "for chunk in test_data_chunks:\n",
    "    # Lấy 20% cuối của mỗi chunk\n",
    "    test_subset = chunk.tail(int(len(chunk) * 0.2))  # Lấy 20% cuối\n",
    "    \n",
    "    # Tạo dataset kiểm tra từ phần dữ liệu con (subset)\n",
    "    test_dataset_subset = tf.data.Dataset.from_generator(\n",
    "        lambda: data_generator(test_subset, feature_cols, target_cols, batch_size=batch_size, timesteps=24),\n",
    "        output_signature=(tf.TensorSpec(shape=(None, timesteps, len(feature_cols)), dtype=tf.float32), \n",
    "                          tf.TensorSpec(shape=(None, 24, len(target_cols)), dtype=tf.float32))\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Đánh giá mô hình trên tập test con (subset)\n",
    "    loss, mae = model.evaluate(test_dataset_subset, verbose=1)\n",
    "    print(f\"✅ Kết quả đánh giá trên dữ liệu test con (chunk): Loss={loss:.4f}, MAE={mae:.4f}\")\n",
    "\n",
    "    # Tính lỗi cho từng mẫu trong tập kiểm tra con\n",
    "    for X_batch, y_batch in test_dataset_subset:\n",
    "        y_true.append(y_batch)\n",
    "        y_pred.append(model.predict(X_batch))\n",
    "\n",
    "# Chuyển danh sách thành mảng numpy\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "# Tính MAE (Lỗi tuyệt đối trung bình) cho toàn bộ tập kiểm tra\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"MAE (Toàn bộ tập kiểm tra con): {mae}\")\n",
    "\n",
    "# Tính MSE và RMSE\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"MSE: {mse}, RMSE: {rmse}\")\n",
    "\n",
    "# Tính R² (R-squared)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f\"R²: {r2}\")\n",
    "\n",
    "# Phân tích lỗi: Tính lỗi tuyệt đối cho mỗi mẫu\n",
    "errors = np.abs(y_true - y_pred)\n",
    "\n",
    "# Vẽ histogram phân phối lỗi để kiểm tra sự phân bố\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=50, color='blue', edgecolor='black')\n",
    "plt.title(\"Phân phối lỗi (MAE)\")\n",
    "plt.xlabel(\"Lỗi tuyệt đối\")\n",
    "plt.ylabel(\"Tần suất\")\n",
    "plt.show()\n",
    "\n",
    "# Phân tích các điểm dữ liệu có lỗi lớn (các điểm có lỗi cao)\n",
    "error_threshold = 0.1  # Bạn có thể điều chỉnh ngưỡng này tùy theo dữ liệu của bạn\n",
    "high_error_indices = np.where(errors > error_threshold)[0]  # Các chỉ số có lỗi lớn\n",
    "\n",
    "# In ra những chỉ số lỗi lớn\n",
    "print(f\"Số lượng mẫu có lỗi lớn: {len(high_error_indices)}\")\n",
    "print(f\"Chỉ số các mẫu có lỗi lớn: {high_error_indices[:10]}\")  # In 10 mẫu đầu tiên có lỗi lớn\n",
    "\n",
    "# Hiển thị các mẫu có lỗi lớn\n",
    "high_error_data = pd.DataFrame({\n",
    "    'y_true': y_true[high_error_indices],\n",
    "    'y_pred': y_pred[high_error_indices],\n",
    "    'error': errors[high_error_indices]\n",
    "})\n",
    "\n",
    "# In ra các lỗi lớn để kiểm tra\n",
    "print(high_error_data.head())\n",
    "\n",
    "# Kiểm tra lỗi theo tọa độ\n",
    "latitude = np.concatenate([X_batch[:, :, 0] for X_batch, _ in test_dataset_subset], axis=0)  # Giả sử latitudes là cột đầu tiên trong X\n",
    "longitude = np.concatenate([X_batch[:, :, 1] for X_batch, _ in test_dataset_subset], axis=0)  # Giả sử longitudes là cột thứ hai trong X\n",
    "\n",
    "# Tạo DataFrame chứa tọa độ và lỗi\n",
    "df = pd.DataFrame({\n",
    "    'Latitude': latitude,\n",
    "    'Longitude': longitude,\n",
    "    'y_true': y_true.flatten(),\n",
    "    'y_pred': y_pred.flatten(),\n",
    "    'error': errors.flatten()\n",
    "})\n",
    "\n",
    "# Tính MAE theo tọa độ (tính MAE cho mỗi cặp latitude, longitude)\n",
    "mae_by_location = df.groupby(['Latitude', 'Longitude'])['error'].mean()\n",
    "\n",
    "# Chuyển kết quả thành DataFrame và reset index để sử dụng pivot\n",
    "mae_by_location = mae_by_location.reset_index()\n",
    "\n",
    "# Vẽ heatmap MAE theo tọa độ\n",
    "mae_by_location_pivot = mae_by_location.pivot(index='Latitude', columns='Longitude', values='error')\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(mae_by_location_pivot, cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Heatmap MAE theo tọa độ\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n",
    "\n",
    "# Kiểm tra các khu vực có MAE cao hơn\n",
    "high_mae_locations = mae_by_location[mae_by_location['error'] > error_threshold]\n",
    "print(\"Các tọa độ có MAE cao:\")\n",
    "print(high_mae_locations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather_lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
